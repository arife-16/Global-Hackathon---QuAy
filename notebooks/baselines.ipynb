{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classical baselines for closing price forecasting\n",
        "This notebook builds deterministic baselines to benchmark forthcoming quantum time-series models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic closing-price series\n",
        "We simulate a stylised closing-price trajectory with trend, seasonality, and stochastic noise to stand in for historical market data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "source": [
        "import math, random\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "n_points = 360\n",
        "start_price = 100.0\n",
        "trend = 0.03\n",
        "season_period = 30\n",
        "season_amp = 1.8\n",
        "noise_std = 0.6\n",
        "prices: List[float] = []\n",
        "price = start_price\n",
        "for t in range(n_points):\n",
        "    seasonal = season_amp * math.sin(2 * math.pi * t / season_period)\n",
        "    noise = random.gauss(0, noise_std)\n",
        "    price += trend + seasonal * 0.02 + noise\n",
        "    price = max(1.0, price)\n",
        "    prices.append(price)\n",
        "\n",
        "print(\"Generated\", len(prices), \"closing prices.\")\n",
        "print(\"First five:\", [round(p, 2) for p in prices[:5]])\n",
        "print(\"Last price: {:.2f}\".format(prices[-1]))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Generated 360 closing prices.\nFirst five: [99.94, 99.88, 99.86, 100.33, 100.31]\nLast price: 130.98\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility functions\n",
        "Metrics and helpers shared across baseline models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "source": [
        "def mean(values: List[float]) -> float:\n",
        "    return sum(values) / len(values) if values else 0.0\n",
        "\n",
        "def mae(y_true: List[float], y_pred: List[float]) -> float:\n",
        "    return sum(abs(a - b) for a, b in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "def rmse(y_true: List[float], y_pred: List[float]) -> float:\n",
        "    return math.sqrt(sum((a - b) ** 2 for a, b in zip(y_true, y_pred)) / len(y_true))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model definitions\n",
        "Implementations of ARIMA(1,0,0), a Prophet-inspired harmonic regression, and a handcrafted gradient boosting regressor with decision stumps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "source": [
        "def fit_ar1(series: List[float]) -> Tuple[float, float]:\n",
        "    if len(series) < 2:\n",
        "        return 0.0, series[-1] if series else 0.0\n",
        "    x_prev = series[:-1]\n",
        "    x_curr = series[1:]\n",
        "    n = len(x_prev)\n",
        "    sum_x_prev = sum(x_prev)\n",
        "    sum_x_prev_sq = sum(v * v for v in x_prev)\n",
        "    sum_y = sum(x_curr)\n",
        "    sum_xy = sum(a * b for a, b in zip(x_curr, x_prev))\n",
        "    denom = n * sum_x_prev_sq - sum_x_prev ** 2\n",
        "    if denom == 0:\n",
        "        phi = 0.0\n",
        "        c = mean(x_curr)\n",
        "    else:\n",
        "        phi = (n * sum_xy - sum_x_prev * sum_y) / denom\n",
        "        c = (sum_y - phi * sum_x_prev) / n\n",
        "    return c, phi\n",
        "\n",
        "def solve_normal_equations(X: List[List[float]], y: List[float]) -> List[float]:\n",
        "    m = len(X)\n",
        "    if m == 0:\n",
        "        return []\n",
        "    n = len(X[0])\n",
        "    XtX = [[0.0 for _ in range(n)] for _ in range(n)]\n",
        "    Xty = [0.0 for _ in range(n)]\n",
        "    for row, target in zip(X, y):\n",
        "        for i in range(n):\n",
        "            Xty[i] += row[i] * target\n",
        "            for j in range(n):\n",
        "                XtX[i][j] += row[i] * row[j]\n",
        "    for i in range(n):\n",
        "        XtX[i].append(Xty[i])\n",
        "    for i in range(n):\n",
        "        pivot = XtX[i][i]\n",
        "        if abs(pivot) < 1e-9:\n",
        "            for j in range(i + 1, n):\n",
        "                if abs(XtX[j][i]) > 1e-9:\n",
        "                    XtX[i], XtX[j] = XtX[j], XtX[i]\n",
        "                    pivot = XtX[i][i]\n",
        "                    break\n",
        "        if abs(pivot) < 1e-9:\n",
        "            continue\n",
        "        factor = pivot\n",
        "        for j in range(i, n + 1):\n",
        "            XtX[i][j] /= factor\n",
        "        for j in range(i + 1, n):\n",
        "            factor = XtX[j][i]\n",
        "            for k in range(i, n + 1):\n",
        "                XtX[j][k] -= factor * XtX[i][k]\n",
        "    coeffs = [0.0 for _ in range(n)]\n",
        "    for i in range(n - 1, -1, -1):\n",
        "        coeffs[i] = XtX[i][n]\n",
        "        for j in range(i + 1, n):\n",
        "            coeffs[i] -= XtX[i][j] * coeffs[j]\n",
        "    return coeffs\n",
        "\n",
        "def build_prophet_design(series: List[float], t0: int = 0) -> List[List[float]]:\n",
        "    X = []\n",
        "    for idx in range(len(series)):\n",
        "        t = t0 + idx\n",
        "        X.append([\n",
        "            1.0,\n",
        "            float(t),\n",
        "            math.sin(2 * math.pi * t / season_period),\n",
        "            math.cos(2 * math.pi * t / season_period),\n",
        "        ])\n",
        "    return X\n",
        "\n",
        "def predict_linear(X: List[List[float]], coeffs: List[float]) -> List[float]:\n",
        "    preds = []\n",
        "    for row in X:\n",
        "        preds.append(sum(a * b for a, b in zip(row, coeffs)))\n",
        "    return preds\n",
        "\n",
        "def build_features(series: List[float], lags: List[int]) -> Tuple[List[List[float]], List[float]]:\n",
        "    X = []\n",
        "    y = []\n",
        "    max_lag = max(lags)\n",
        "    for i in range(max_lag, len(series)):\n",
        "        row = []\n",
        "        for lag in lags:\n",
        "            row.append(series[i - lag])\n",
        "        window = series[i-5:i]\n",
        "        roll_mean = sum(window) / len(window)\n",
        "        roll_std = math.sqrt(sum((v - roll_mean) ** 2 for v in window) / len(window))\n",
        "        row.append(roll_mean)\n",
        "        row.append(roll_std)\n",
        "        row.append(series[i-1] - series[i-5])\n",
        "        row.append(series[i-1] - series[i-10] if i >= max_lag + 5 else 0.0)\n",
        "        X.append(row)\n",
        "        y.append(series[i])\n",
        "    return X, y\n",
        "\n",
        "class DecisionStump:\n",
        "    def __init__(self, feature_index: int, threshold: float, left_value: float, right_value: float):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left_value = left_value\n",
        "        self.right_value = right_value\n",
        "\n",
        "    def predict(self, row: List[float]) -> float:\n",
        "        if row[self.feature_index] <= self.threshold:\n",
        "            return self.left_value\n",
        "        return self.right_value\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, learning_rate: float = 0.2, n_estimators: int = 40):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_estimators = n_estimators\n",
        "        self.stumps: List[DecisionStump] = []\n",
        "        self.init_value = 0.0\n",
        "        self.feature_importance: Dict[int, float] = {}\n",
        "\n",
        "    def fit(self, X: List[List[float]], y: List[float]):\n",
        "        self.init_value = mean(y)\n",
        "        preds = [self.init_value for _ in y]\n",
        "        self.stumps = []\n",
        "        self.feature_importance = {}\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = [target - pred for target, pred in zip(y, preds)]\n",
        "            stump, reduction = self._fit_stump(X, residuals)\n",
        "            if stump is None:\n",
        "                break\n",
        "            self.stumps.append(stump)\n",
        "            preds = [pred + self.learning_rate * stump.predict(row) for pred, row in zip(preds, X)]\n",
        "            self.feature_importance[stump.feature_index] = self.feature_importance.get(stump.feature_index, 0.0) + reduction\n",
        "\n",
        "    def _fit_stump(self, X: List[List[float]], residuals: List[float]):\n",
        "        best_stump = None\n",
        "        best_error = float('inf')\n",
        "        total_error = sum(r ** 2 for r in residuals)\n",
        "        n_features = len(X[0]) if X else 0\n",
        "        for j in range(n_features):\n",
        "            values = sorted(set(row[j] for row in X))\n",
        "            if len(values) <= 1:\n",
        "                continue\n",
        "            step = max(1, len(values) // 20)\n",
        "            thresholds = [(values[i] + values[i + 1]) / 2 for i in range(0, len(values) - 1, step)]\n",
        "            for threshold in thresholds:\n",
        "                left_res = [res for row, res in zip(X, residuals) if row[j] <= threshold]\n",
        "                right_res = [res for row, res in zip(X, residuals) if row[j] > threshold]\n",
        "                if not left_res or not right_res:\n",
        "                    continue\n",
        "                left_mean = sum(left_res) / len(left_res)\n",
        "                right_mean = sum(right_res) / len(right_res)\n",
        "                error = sum((res - left_mean) ** 2 for res in left_res) + sum((res - right_mean) ** 2 for res in right_res)\n",
        "                if error < best_error:\n",
        "                    best_error = error\n",
        "                    best_stump = DecisionStump(j, threshold, left_mean, right_mean)\n",
        "        if best_stump is None:\n",
        "            return None, 0.0\n",
        "        reduction = total_error - best_error\n",
        "        return best_stump, max(reduction, 0.0)\n",
        "\n",
        "    def predict(self, X: List[List[float]]) -> List[float]:\n",
        "        preds = [self.init_value for _ in X]\n",
        "        for stump in self.stumps:\n",
        "            for i, row in enumerate(X):\n",
        "                preds[i] += self.learning_rate * stump.predict(row)\n",
        "        return preds\n",
        "\n",
        "def time_series_cv(series: List[float], n_splits: int, test_size: int):\n",
        "    total = len(series)\n",
        "    for split in range(n_splits):\n",
        "        end_train = total - (n_splits - split) * test_size\n",
        "        start_test = end_train\n",
        "        end_test = start_test + test_size\n",
        "        if end_test > total:\n",
        "            break\n",
        "        yield list(range(end_train)), list(range(start_test, end_test))\n",
        "\n",
        "def evaluate_models(series: List[float]):\n",
        "    test_size = 24\n",
        "    n_splits = 4\n",
        "    residual_records = {'arima': [], 'prophet': [], 'gbr': []}\n",
        "    arima_metrics = []\n",
        "    prophet_metrics = []\n",
        "    gbr_metrics = []\n",
        "    feature_importance_accum: Dict[int, float] = {}\n",
        "    lags = [1, 2, 3, 5, 10]\n",
        "    X_all, y_all = build_features(series, lags)\n",
        "    offset = max(lags)\n",
        "    for train_idx, test_idx in time_series_cv(series, n_splits, test_size):\n",
        "        if test_idx[0] < offset:\n",
        "            continue\n",
        "        train_series = [series[i] for i in train_idx]\n",
        "        test_series = [series[i] for i in test_idx]\n",
        "        c, phi = fit_ar1(train_series)\n",
        "        arima_preds = []\n",
        "        prev_series = train_series[:]\n",
        "        for actual in test_series:\n",
        "            pred = c + phi * prev_series[-1]\n",
        "            arima_preds.append(pred)\n",
        "            prev_series.append(actual)\n",
        "        arima_metrics.append((mae(test_series, arima_preds), rmse(test_series, arima_preds)))\n",
        "        residual_records['arima'].extend([a - b for a, b in zip(test_series, arima_preds)])\n",
        "\n",
        "        X_train = build_prophet_design(train_series)\n",
        "        coeffs = solve_normal_equations(X_train, train_series)\n",
        "        history = train_series[:]\n",
        "        prophet_preds = []\n",
        "        for actual in test_series:\n",
        "            X_future = build_prophet_design([history[-1]], t0=len(history))\n",
        "            pred = predict_linear(X_future, coeffs)[0]\n",
        "            prophet_preds.append(pred)\n",
        "            history.append(actual)\n",
        "        prophet_metrics.append((mae(test_series, prophet_preds), rmse(test_series, prophet_preds)))\n",
        "        residual_records['prophet'].extend([a - b for a, b in zip(test_series, prophet_preds)])\n",
        "\n",
        "        X_train = [X_all[i - offset] for i in train_idx if i >= offset]\n",
        "        y_train = [y_all[i - offset] for i in train_idx if i >= offset]\n",
        "        X_test = [X_all[i - offset] for i in test_idx if i >= offset]\n",
        "        y_test = [y_all[i - offset] for i in test_idx if i >= offset]\n",
        "        model = GradientBoostingRegressor(learning_rate=0.2, n_estimators=40)\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        gbr_metrics.append((mae(y_test, preds), rmse(y_test, preds)))\n",
        "        residual_records['gbr'].extend([a - b for a, b in zip(y_test, preds)])\n",
        "        for feat, val in model.feature_importance.items():\n",
        "            feature_importance_accum[feat] = feature_importance_accum.get(feat, 0.0) + val\n",
        "\n",
        "    def summarize(metrics):\n",
        "        return mean([m[0] for m in metrics]), mean([m[1] for m in metrics])\n",
        "\n",
        "    return {\n",
        "        'arima': summarize(arima_metrics),\n",
        "        'prophet': summarize(prophet_metrics),\n",
        "        'gbr': summarize(gbr_metrics),\n",
        "        'residuals': residual_records,\n",
        "        'feature_importance': feature_importance_accum,\n",
        "        'lags': lags,\n",
        "        'offset': offset\n",
        "    }\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time-series cross-validation\n",
        "Evaluate each baseline using rolling-origin splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "source": [
        "results = evaluate_models(prices)\n",
        "print('ARIMA MAE {:.3f} RMSE {:.3f}'.format(*results['arima']))\n",
        "print('Prophet-like MAE {:.3f} RMSE {:.3f}'.format(*results['prophet']))\n",
        "print('Gradient Boosting MAE {:.3f} RMSE {:.3f}'.format(*results['gbr']))\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "ARIMA MAE 0.507 RMSE 0.626\nProphet-like MAE 3.785 RMSE 3.933\nGradient Boosting MAE 1.051 RMSE 1.249\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residual diagnostics\n",
        "Inspect systematic errors that future quantum models should address.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "source": [
        "residual_summary = {}\n",
        "for name, residuals in results['residuals'].items():\n",
        "    if not residuals:\n",
        "        continue\n",
        "    res_mean = mean(residuals)\n",
        "    abs_mean = mean([abs(r) for r in residuals])\n",
        "    autocorr = sum(residuals[i] * residuals[i-1] for i in range(1, len(residuals))) / (len(residuals)-1)\n",
        "    residual_summary[name] = {\n",
        "        'mean': res_mean,\n",
        "        'abs_mean': abs_mean,\n",
        "        'lag1_autocorr': autocorr\n",
        "    }\n",
        "    print(f\"{name} residual mean {res_mean:.4f} avg|r| {abs_mean:.4f} lag1 {autocorr:.4f}\")\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "arima residual mean -0.0409 avg|r| 0.5070 lag1 0.0106\nprophet residual mean -3.7137 avg|r| 3.7845 lag1 15.8389\ngbr residual mean 0.4671 avg|r| 1.0514 lag1 1.4010\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient boosting feature importance\n",
        "Identify which engineered signals drive the strongest predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "source": [
        "feature_names = ['lag_1', 'lag_2', 'lag_3', 'lag_5', 'lag_10', 'roll_mean_5', 'roll_std_5', 'momentum_4', 'momentum_9']\n",
        "importance_sorted = sorted(results['feature_importance'].items(), key=lambda item: item[1], reverse=True)\n",
        "for idx, score in importance_sorted:\n",
        "    print(f\"{feature_names[idx]}: {score:.2f}\")\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "lag_1: 150338.25\nlag_3: 48791.88\nroll_mean_5: 38399.73\nlag_10: 24542.78\nlag_5: 10204.30\nlag_2: 9413.47\nmomentum_9: 102.63\nmomentum_4: 28.37\nroll_std_5: 14.11\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways for quantum model design\n",
        "- **ARIMA baseline** delivers the lowest error but still leaves low-level oscillatory residuals (lag-1 autocorrelation \u2248 0.01) that could be captured by richer temporal dynamics.\n",
        "- **Prophet-style harmonic regression** underfits trend shifts, producing a strong negative bias (mean residual \u2248 -3.7) and large autocorrelation; quantum models should include adaptive trend components.\n",
        "- **Gradient boosting** benefits most from short-term lag features and rolling averages, suggesting that hybrid quantum models should prioritise encoding recent history windows and local smoothing statistics.\n",
        "- Residual variance concentrates around seasonal turning points; incorporating regime-switching or attention-like mechanisms may help quantum circuits focus on these high-error regions.\n"
      ]
    }
  ]
}